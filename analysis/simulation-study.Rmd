---
title: "Simulation study"
author: "Shengtong Han"
output: html_document
---

```{r, echo=F}
library(ggplot2)
library(gridExtra)
```


```{r, echo=F}
loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    get(ls()[ls() != "fileName"])
}
```



<!-- Add your analysis here -->
## Data Generation

Basically, there are two sequential levels when generating the data. First under null hypothesis, generate total number of variants in cases and controls, i.e., AF $q_{ij}^{(0)} \sim Beta(\alpha_0, \beta_0)$, $X_{1ij}+X_{0ij} \sim Pois (N1+N0, q_{ij}^{(0)})$,  filter out variants will null mutations, then re-distribute all mutations in effective variants into cases and controls depending on the risk status via conditional binomial distribution.     

* gene level: $U_i$ denote the risk status of gene $i$, $U_i \sim Ber(1, \delta)$. All genes share the same risk probability $\delta$. 
* variant level:
    +  $U_i=0$, all variants for gene $i$ are non-causal, which is under null hypothesis. Given the generated $X_{1ij}+X_{0ij}$, split into case and controls by conditional binomial distribution, with $p=\frac{N1}{N1+N0}$  
    + $U_i=1$,  gene $i$ is a causal gene, the variant $(i,j)$ whose risk status is denoted by $Z_{ij}$ is generated as $Z_{ij} \sim Ber(1, \pi(\eta))$, $\pi(\eta)$ is a function linking annotations to the probability of being causal. 
          *  $Z_{ij}=0$ (which is under null),  given the generated $X_{1ij}+X_{0ij}$, split into case and controls by conditional binomial distribution, with $p=\frac{N1}{N1+N0}$
          *   $Z_{ij}=1$ (which is under alternative), AF $q_{ij}^{(1)} \sim Beta(\alpha, \beta)$, $\gamma_{ij} \sim Gamma(\bar{\gamma}*\sigma, \sigma)$,  $X_{1ij}+X_{0ij} \sim Pois(N1+N0, q_{ij}^{(1)})$, split into case and control by conditional binomial distribution with $p=\frac{N1*\gamma_{ij}}{N1*\gamma_{ij}+N0}$. 
          
Filtering step: filter variants that have null variant counts in both cases and controls.  

Notations: 

* N1, N0 are sample sizes in cases and controls 
* $q_{ij}$ is allele frequency for variant $(i,j)$
* $\bar{\gamma}$ relative risk 


$\pi(\eta)$ is the link function between annotations and prior probability and the interest is in estimating $\eta$.  

## Disjoint group priors 

### Case 1: All genes are risk genes



#### (1) only estimate $\eta$, fix $\delta=1$ 


* relative risk effect, $\bar{\gamma}$ on estimating $\eta$ 

Parameter settings: N1=N0=3000, $\alpha=\alpha_0=0.1$, $\beta=2000, \beta_0=1000$. 100 genes, each having 1000 variants with $\eta_i=0.1$ risk variants, but the actural $\eta_i$ is varying for each generated data set because of the filtering. $\bar{\gamma}=2,3,4,5$ and with each $\bar{\gamma}$, perform 100 independent runs. 



```{r, echo=F}
num.gamma=4
all.eta=matrix(nrow=100, ncol=num.gamma)
bargamma_2=load("../output/Varybargamma/gammabar_2.RData")
all.eta[,1]=all.pi[,1]
bargamma_3=load("../output/Varybargamma/gammabar_3.RData")
all.eta[,2]=all.pi[,1]
bargamma_4=load("../output/Varybargamma/gammabar_4.RData")
all.eta[,3]=all.pi[,1]
bargamma_5=load("../output/Varybargamma/gammabar_5.RData")
all.eta[,4]=all.pi[,1]
#bargamma_6=load("../output/Varybargamma/bargamma_6.RData")
#all.eta[,5]=all.pi[,1]
colnames(all.eta)=2:(2+num.gamma-1)

eta.mean=c(mean(all.eta[which(all.eta[,1]>0),1]), mean(all.eta[which(all.eta[,2]>0),2]), mean(all.eta[which(all.eta[,3]>0),3]), mean(all.eta[which(all.eta[,4]>0),4]))
eta.sd=c(sd(all.eta[which(all.eta[,1]>0),1]), sd(all.eta[which(all.eta[,2]>0),2]), sd(all.eta[which(all.eta[,3]>0),3]), sd(all.eta[which(all.eta[,4]>0),4]))
```



```{r, echo=F}
bar.gamma=2:5
eta.summary=data.frame(mean=eta.mean, sd=eta.sd, bar.gamma=bar.gamma)
p1=ggplot(eta.summary, aes(x=bar.gamma, y=mean, group=1)) + 
   geom_point() +
  geom_pointrange(aes(ymin=mean-sd, ymax=mean+sd))+
  ggtitle("N1=N0=3000")+
  theme(plot.title = element_text(hjust = 0.5, size=10))+  #center the title 
  xlab(expression(paste(bar(gamma))))+
  ylab(expression(paste(hat(eta))))+ 
  guides(fill=guide_legend(title=NULL))+
  ylim(0.05,0.15)+
   geom_hline(yintercept=0.1,linetype="dashed", color = "red")
```  
  


* sample size effect on estimating $\eta$

Parameter settings: N1=N0=1000, 3000, 5000, $\alpha=\alpha_0=0.1$, $\beta=2000, \beta_0=1000$. 100 genes, each having 1000 variants with $\eta_i=0.1$ risk variants, but the actural $\eta_i$ is varying for each generated data set because of the filtering. $\bar{\gamma}=5$.   For each fixed sample size,  perform 100 independent runs. 



```{r, echo=F}
num.sample.size=3
all.eta=matrix(nrow=100, ncol=num.sample.size)
load("../output/Varysamplesize/sample.size.1000.RData")
all.eta[,1]=all.pi[,1]
load("../output/Varysamplesize/sample.size.3000.RData")
all.eta[,2]=all.pi[,1]
load("../output/Varysamplesize/sample.size.5000.RData")
all.eta[,3]=all.pi[,1]
#bargamma_6=load("../output/Varybargamma/bargamma_6.RData")
#all.eta[,5]=all.pi[,1]
colnames(all.eta)=c(1000, 3000, 5000)

eta.mean=c(mean(all.eta[which(all.eta[,1]>0),1]), mean(all.eta[which(all.eta[,2]>0),2]), mean(all.eta[which(all.eta[,3]>0),3]))
eta.sd=c(sd(all.eta[which(all.eta[,1]>0),1]), sd(all.eta[which(all.eta[,2]>0),2]), sd(all.eta[which(all.eta[,3]>0),3]))
```



```{r, echo=F}
library(grid)
sample.size=c(1000, 3000, 5000)
eta.summary=data.frame(mean=eta.mean, sd=eta.sd, sample.size=sample.size)
p2=ggplot(eta.summary, aes(x=sample.size, y=mean, group=1)) + 
   geom_point() +
  geom_pointrange(aes(ymin=mean-sd, ymax=mean+sd))+
  ggtitle(expression(paste(bar(gamma), "=5")))+
  theme(plot.title = element_text(hjust = 0.5, size=10))+  # center the title 
  xlab(expression(paste("N1=N0 ")))+
  ylab(expression(paste(hat(eta))))+ 
  guides(fill=guide_legend(title=NULL))+
  ylim(0.05,0.15)+
   geom_hline(yintercept=0.1,linetype="dashed", color = "red")
grid.arrange(p1, p2, top=grid.text(expression(paste("Estimate of ", eta," across 100 replicates"))), bottom=grid.text(expression(paste(alpha, "=0.1,", alpha[0], "=0.1,", beta, "=2000,", beta[0], "=1000,", eta[i], "=0.1"))), nrow=1)
```  



* power comparison with other methods 


Parameter settings: N1=N0=3000 $\alpha=\alpha_0=0.1$, $\beta=2000, \beta_0=1000$. 100 genes, each having 1000 variants with $\eta_i=0.1, 0.2, 0.3$ risk variants, but the actural $\eta_i$ is varying for each generated data set because of the filtering. $\bar{\gamma}=5$.  For every $\eta_i$, perform 100 independent runs. 


For every simulated risk gene, three different methods are used to test if it is a risk gene or not. Significance level of 0.05 is used as cutoff. The proportion of risk genes correctly detected is plotted.   Burnden test looks at the variant in cases and controls and performs the enrichment analysis. SKATO tests the significance of effect size and generate p value. MIRAGE first estimates the parameter $\eta$, then does a likelihood ratio test and returns p value. 


```{r, echo=F, warning=F}
library(matrixStats)
load("../output/Varyeta/eta_0.1.RData")
TP_eta_0.1=TP
TP_eta_0.1_effect=TP_eta_0.1[apply(TP_eta_0.1!=0, 1, all),] 
load("../output/Varyeta/eta_0.2.RData")
TP_eta_0.2=TP
TP_eta_0.2_effect=TP_eta_0.2[apply(TP_eta_0.2!=0, 1, all),] 
load("../output/Varyeta/eta_0.3.RData")
TP_eta_0.3=TP
TP_eta_0.3_effect=TP_eta_0.3[apply(TP_eta_0.3!=0, 1, all),] 
eta.mean=c(colMeans(TP_eta_0.1_effect), colMeans(TP_eta_0.2_effect), colMeans(TP_eta_0.3_effect))
eta.sd=c(colSds(TP_eta_0.1_effect), colSds(TP_eta_0.2_effect), colSds(TP_eta_0.3_effect))
method=rep(c("SIRAGE", "Burden", "SKATO"),each=3)
```


```{r, echo=F}
eta=rep(c(0.1, 0.2, 0.3),3)
eta.summary=data.frame(eta=eta, mean=c(eta.mean[c(1,4,7)], eta.mean[c(2,5,8)], eta.mean[c(3,6,9)]), sd=c(eta.sd[c(1,4,7)], eta.sd[c(2,5,8)], eta.sd[c(3,6,9)]), method=method)
ggplot(eta.summary, aes(x=eta, y=mean, color=method, group=method)) + 
   geom_line() +
   geom_point()+
  geom_pointrange(aes(ymin=mean-sd, ymax=mean+sd))+
  xlab(expression(paste(eta[i])))+
  ylab("Estimated proportion of risk genes")
  #xlab(expression(paste(bar(gamma), "=5;", "N1=N0 ")))+
  #ylab(expression(paste(hat(eta))))+ 
  #guides(fill=guide_legend(title=NULL))+
  #ylim(0.05,0.15)+
  # geom_hline(yintercept=0.1,linetype="dashed", color = "red")
```


#### (2) estimate both $\beta, \delta$




### Case 2: Mixture model





## Logistic priors  
### Case 1: One annotation feature
#### Paramster settings 1 : 

* $\delta=1$
* $N1=N0=5000$
* number of genes $I=1000$, number of variants per gene $m=200$ before filtering. 
* $\alpha=\alpha_0=0.1$, $\beta_0=1000, \beta=2000$. 
* $\beta_{true}=0.5$,  $\bar{\gamma}=10$
* 80\% elements of of Ajk are 1

This is the simplest case in that every variant has one annotation feature if any. 


#### likelihood function: 
If the feature has one dimension, the objective likelihood function would be simple to be optimized using common R packages. The log likelihood as a function of $\beta$ (true $\beta=0.5$)is as 



![loglikelihood](figure/loglikelihood.png). 

This plot tells us the likelihood only has one mode in one dimension case. It looks like $\beta$ is overestimated by R package-BFGS. With the fixed randomly generated data, run BFGS 50 times with random initials. The mean of $\hat{\beta}_{MLE}=0.7631$ and SD=2.344796e-05. 


#### Effect of Sparsity of $A_{jk}$ ($k$ is the number of features, here $k=1$) on MLE of $\beta$

Set $\beta_{true}=0.5$, vary the sparsity of $A_{jk}$, i.e., the ratio of 0's in $A_{jk}$. Define sparsity rate as the ratio of 0's in each feature category.  


Sparsity rate | 90% | 80% |70%  |60% |50% |40%  | 30% |20% |10% | 0|
-------------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$ |0.3802 | 0.4499 |0.5845 |0.5420 |0.5829 |0.64444|0.6817|0.7608|0.7941|0.8896|
Table: MLE at different sparsity rate $\beta_{true}=0.5$

When All entries of $A_{jk}$ are zero, $\tau_j=\frac{1}{2}$, regardless of whatever values of $\beta$ and the likelihood function is a constant of $\beta$. Thus $\widehat{\beta}$ can be anywhere.   


Set $\beta_{true}=0.2$.  


Sparsity rate | 95% | 90% |85%  |80% |75% |70%  | 65% |60% |55% | 50%|
-------------|-------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$|0.3075 | 0.2259 |0.3481 |0.3944 |0.3677 |0.4167|0.3956|0.4331|0.4565|0.4091|
Table: MLE at different sparsity rate with  $\beta_{true}=0.2$. 

####Important Note: 
There are two variables affecting estimate of $\beta$, sparsity of $A_{jk}$ and $\beta_{true}$. 
(1) $\beta$ cannot be set too large, say >10 because it controls the probability of a variant of being causal. Consider an extreme case where all variants are truly causal risk variants. The likelihood of the data is close to the product of Bayes factor of every variant, free of $\beta$ value. Any large enough $\beta$ will give the same likelihood. (2) Sparsity of $A_{jk}$ will certainly influences the estimate of $\beta$. The less sparsity, the more information will be used, the more accurate the estimate will be. (3) AF cannot be too large because for common variants with large odds ratio (large $\bar{\gamma}$), Bayes factor will become overflow of being Inf.      

####Parameter setting 2-increasing sample size
* $\delta=1$
* $\beta_{true}=2$, $\bar{\gamma}=10$
* all entries of $A_{jk}$ are 1
* $\alpha=\alpha_0=0.1$, $\beta=1000, \beta_0=2000$



N1=N0|10,000 | 50,000 | 100,000|
--------|-------|--------|-----------------------------------------
 $\widehat{\beta}_{MLE}$ by Optim | 2.7810 (0.0362) | 2.4413 (0.0161)| 2.3420 
$\widehat{\beta}_{MLE}$ by GLM | 2.0016  (0.0126)|
Table: MLE averaging across 10 replicates with different sample size 


To use GLM, $Z_{ij}$ is treated as response (but in practice, $Z_{ij}$ is unknown) and each column of $A_{jk}$ is a predictor, but without intercept.  

Use function GLM to estimate $\beta$. 


### Case 2: Two Annotation features 
$K=2$. Set $\beta_1=0.05, \beta_2=2$. Other parameters are 

* $\delta=1$
* $\bar{\gamma}=10$
* $\alpha=\alpha_0=0.1$, $\beta=1000, \beta_0=2000$

When there are more than 2 groups, $A_{ij}$ must be designed carefully such that all columns (all annotations) have as little overlap as possible. Otherwise, the method have difficulty in distinguishing between them, leading to poor estimate. In simulations below, these two factures have no overlap. 



N1=N0|10,000 | 50,000 | 80,000|
--------|-------|--------|-----------------------------------------
 $\widehat{\beta_1}_{MLE}$ by Optim | 0.3472 (0.0239) | 0.2287 (0.0252)|0.2371 (0.0193)  
 $\widehat{\beta_2}_{MLE}$ by Optim | 2.7807 (0.0919) | 2.4239 (0.0211)| 2.3636 (0.0243)
$\widehat{\beta_1}_{MLE}$ by GLM | 0.0517  (0.0159)| |
$\widehat{\beta_2}_{MLE}$ by GLM | 1.9952  (0.0177)||
Table: MLE averaging across 10 replicates with different sample size 


### Case 3:  Many annotation features


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
