---
title: "Simulation study"
author: "Shengtong Han"
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```
<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->
## Data Generation

Basically, there are two sequential levels when generating the data

* gene level: $U_i$ denote the risk status of gene $i$, $U_i \sim Ber(1, \delta)$. All genes share the same risk probability $\delta$. 
* variant level:
    +  $U_i=0$, all variants for gene $i$ are non-causal, which is under null hypothesis. AF $q_{ij} \sim Beta(\alpha_0, \beta_0)$, $X_{1ij}+X_{0ij} \sim Pois (N1+N0, q_{ij})$, (alternatively, $X_{1ij} \sim Binom(N1, q_{ij}); X_{0ij} \sim Binom(N0, q_{ij})$); then filter out  variants if $X_{1ij}+X_{0ij}=0$.  
    + $U_i=1$,  gene $i$ is a causal gene, the variant $(i,j)$ whose risk status is denoted by $Z_{ij}$ is generated as $Z_{ij} \sim Ber(1, \pi(\beta))$, $\pi(\beta)$ is a function linking annotations to the probability of being causal. 
          *  $Z_{ij}=0$ (which is under null),  given the generated $X_{1ij}+X_{0ij}$, split into case and controls by conditional binomial distribution, with $p=\frac{N1}{N1+N0}$
          *   $Z_{ij}=1$ (which is under alternative), AF $q_{ij} \sim Beta(\alpha, \beta)$, $\gamma_{ij} \sim Gamma(\bar{\gamma}*\sigma, \sigma)$, given generated  $X_{1ij}+X_{0ij}$, split into case and control by conditional binomial distribution with $p=\frac{N1*\gamma_{ij}}{N1*\gamma_{ij}+N0}$. 
          
Filtering step: filter variants that have null variant counts in both cases and controls.  

Notations: 

* N1, N0 are sample sizes in cases and controls 
* $q_{ij}$ is allele frequency for variant $(i,j)$
* $\bar{\gamma}$ relative risk 


$\pi(\beta)$ is the link function between annotations and prior probability and the interest is in estimating $\beta$.  
```{r data generation, echo=F}
gene.simu=function(N0, N1, m, alpha0, beta0, alpha, beta, gamma.mean, sigma, pi, model, num.group, split.ratio)
{
  pheno=c(rep(0,N0), rep(1,N1))  # filtering step
  q <- rbeta(m, alpha0, beta0)
  x1x0=matrix(nrow=(N0+N1), ncol=m)
  for (i in 1:m)  # generate the data with certain variants full of zeros
    x1x0[,i]=rpois((N0+N1), q[i])
  filter.x1x0=x1x0[,!apply(x1x0==0,2,all)]  # filter variants with 0 counts in both cases and controls since these variants are noninformative
  mm=ncol(filter.x1x0)
  var.index=which(colSums(x1x0!=0)>0) # get the variant index in the original data before filtering
  
  ##################
  #mm=m
  gamma <- rep(1,mm)
  Zij=rep(0,mm)
  qq=rbeta(mm, alpha0, beta0)
  if (model==1)
  {
  #  for (k in 1:num.group)
  #  {
  #    from=split.ratio[k]*mm+1; to=split.ratio[k+1]*mm
  #    Zij[from:to]=rbinom((to-from+1), 1, pi[k])
  #  }
    for (i in 1:mm)
      Zij[i]=rbinom(1,1,pi[var.index[mm]])
    
    qq[Zij==1] <- rbeta(sum(Zij==1), alpha, beta)
    gamma[Zij==1] <- rgamma(sum(Zij==1), gamma.mean*sigma, sigma)
  }
  
  x <- array(0, dim=c(length(pheno), mm))
  for (j in 1:mm)
  {
    x1=rbinom(1, sum(filter.x1x0[,j]), N1*gamma[j]/(gamma[j]*N1+N0))
    #x1=sum(filter.x1x0[,j])-x0
    x0=sum(filter.x1x0[,j])-x1
    x[sample.int(N0,x0),j]=1
    x[N0+sample.int(N1,x1),j]=1
  }
  x=as.matrix(x)
  return (list(geno=x,pheno=pheno,q=q,gamma=gamma, Zij=Zij, var.index=var.index))
  
}
```

```{r calculate BF of single variant by integration, echo=F}
intergrand=function(aa, var.case, var.contr, bar.gamma, sig, N1, N0)
{
  ff=dbinom(var.case, sum(var.case, var.contr), aa*N1/(aa*N1+N0))*dgamma(aa, bar.gamma*sig, sig)
  return(ff)
}
# calculate the bayes factor of a single variant via integration
BF.var.inte=function(var.case, var.contr, bar.gamma, sig, N1, N0)
{
  marglik0.CC <- dbinom(var.case, sum(var.case, var.contr), N1/(N1+N0))    # Under H0: gamma=1
  
  marglik1.CC <- integrate(intergrand, var.case, var.contr, bar.gamma, sig, N1, N0, low=0, upper=100, stop.on.error=F)$value # Under H1: gamma~gamma(gamma.mean*sigma, sigma) 
  BF.var <- marglik1.CC/marglik0.CC
  
  return(BF.var)
}
```

```{r objective functions, echo=F}
objec.func=function(beta.est) # this is the log likelihood function to be maximized
{
  lkhd=0
  for (i in 1:num.var)
    lkhd=lkhd+log(1+BF.var[i]*exp(Ajk.effect[i]*beta.est))-log(1+exp(Ajk.effect[i]*beta.est))
  
  lkhd=lkhd-lambda/2*sum(beta.est^2)
  
  return(lkhd)  
}

objec.func.min=function(beta.est) # this is the log likelihood function to be maximized
{
  lkhd=0
  for (i in 1:num.var)
    lkhd=lkhd-log(1+BF.var[i]*exp(Ajk.effect[i]*beta.est))+log(1+exp(Ajk.effect[i]*beta.est))
  
  lkhd=lkhd+lambda/2*sum(beta.est^2)
  
  return(lkhd)  
}

deriv.objec.func=function(beta.est)
{
  par.beta=numeric(anno.num)
  for (k in 1:anno.num) # k: annotattion index
  {
    par.beta.k=0
    for (j in 1:num.var) # j: variant index
    {
      delta.j=exp(Ajk.effect[j]*beta.est)
      par.beta.k=par.beta.k+Ajk.effect[j]*delta.j*(BF.var[j]/(1+delta.j*BF.var[j])-1/(1+delta.j))
    }  
    par.beta[k]=par.beta.k-lambda*beta.est[k] # k th element of first derivative
  }
  
  return(par.beta)
}
```

```{r parameter settings, echo=F}
num.gene=1000
m=200
N0=5000; N1=5000
delta=1
alpha0 <- 0.1
beta0 <- 1000
alpha <- 0.1
beta <- 2000
gamma.mean <- 10
sigma <- 1
split.ratio=c(0,1)
num.group=length(split.ratio)-1
anno.num=1
```

## Parameter estimation 
### Case 1: One annotation feature
#### Paramster settings: 
This is the simplest case in that every variant has one annotation feature if any. Set $\beta=0.5$,  $\bar{\gamma}=10$ to make signal strong with burden close to 10; 80\% elements of of Ajk are 1. 


#### likelihood function: 
If the feature has one dimension, the objective likelihood function would be simple to be optimized using common R packages. The log likelihood as a function of $\beta$ (true $\beta=0.5$)is as ![loglikelihood](figure/loglikelihood.png). 

This plot tells us the likelihood only has one mode in one dimension case. It looks like $\beta$ is overestimated by R package-BFGS. With the fixed randomly generated data, run BFGS 50 times with random initials. The mean of $\hat{\beta}_{MLE}=0.7631$ and SD=2.344796e-05. 


#### Effect of Sparsity of $A_{jk}$ ($k$ is the number of features, here $k=1$) on MLE of $\beta$

Set $\beta_{true}=0.5$, vary the sparsity of $A_{jk}$, i.e., the ratio of 0's in $A_{jk}$. Define sparsity rate as the ratio of 0's in each feature category.  


Sparsity rate | 90% | 80% |70%  |60% |50% |40%  | 30% |20% |10% | 0|
-------------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$ |0.3802 | 0.4499 |0.5845 |0.5420 |0.5829 |0.64444|0.6817|0.7608|0.7941|0.8896|

When All entries of $A_{jk}$ are zero, $\tau_j=\frac{1}{2}$, regardless of whatever values of $\beta$ and the likelihood function is a constant of $\beta$. Thus $\widehat{\beta}$ can be anywhere.   


Set $\beta_{true}=0.2$.  


Sparsity rate | 95% | 90% |85%  |80% |75% |70%  | 65% |60% |55% | 50%|
-------------|-------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$|0.3075 | 0.2259 |0.3481 |0.3944 |0.3677 |0.4167|0.3956|0.4331|0.4565|0.4091|

####Important Note: 
There are two variables affecting estimate of $\beta$, sparsity of $A_{jk}$ and $\beta_{true}$. 
(1) $\beta$ cannot be set too large, say >10 because it controls the probability of a variant of being causal. Consider an extreme case where all variants are truly causal risk variants. The likelihood of the data is close to the product of Bayes factor of every variant, free of $\beta$ value. Any large enough $\beta$ will give the same likelihood. (2) Sparsity of $A_{jk}$ will certainly influences the estimate of $\beta$. The less sparsity, the more information will be used, the more accurate the estimate will be.    

####Introducing  $\ell_2$ penalty. 
The same parameter setting. Sparsity rate is 20%. $\beta_{true}=0.5$. 
![betaandpenalty](figure/PenaltyBeta.png)


With this setting, very large tunning parameter $\lambda>1000$ should be used to get the good estimate of $\beta$.  


```{r main algorithm, echo=T, results="hide", cache=T, eval=F}
Ajk=rep(0, (num.gene*m))
Ajk[sample(length(Ajk), (0.8*length(Ajk)))]=1
############## generate beta 
beta.true=numeric(anno.num)
beta.true[1]=0.5
############# compute tau: risk of every variant being risk variant
tau.true=exp(Ajk*beta.true)/(1+exp(Ajk*beta.true))
Ui=rbinom(num.gene, 1, delta)
  all.data=list(); var.orig.index=numeric(); var.orig.index[1]=NA
  for (i in 1:num.gene)
  {
    data=gene.simu(N0, N1, m, alpha0, beta0, alpha, beta, gamma.mean, sigma, pi=tau.true[((i-1)*m+1):(i*m)], Ui[i], num.group, split.ratio)
    all.data[[i]]=data
    var.orig.index=c(var.orig.index, (i-1)*m+data$var.index)
  }
  var.orig.index=var.orig.index[-1]
  Ajk.effect=Ajk[var.orig.index]
  ################### calculate bayes factor for every variant 
  k=0; BF.var=numeric(); var.count=matrix(nrow=(m*num.gene), ncol=2)
  for (i in 1:length(all.data))
  {
    for (j in 1:ncol(all.data[[i]]$geno))
    {
      k=k+1
      BF.var[k]=BF.var.inte(sum(all.data[[i]]$geno[all.data[[i]]$pheno==1,j]), sum(all.data[[i]]$geno[all.data[[i]]$pheno==0,j]), bar.gamma=6, sig=sigma, N1, N0)
      var.count[k,]=c(sum(all.data[[i]]$geno[all.data[[i]]$pheno==1,j]),sum(all.data[[i]]$geno[all.data[[i]]$pheno==0,j]) )
    }
  } # end of i
 var.count=var.count[-((k+1):nrow(var.count)),]
 ###############################
 ################################
  num.var=length(BF.var)
  lambda.range=rep(0, 50)
  error=numeric()
  beta.fin.est=numeric(length(lambda.range))
###################################
  lambda=0
  beta.range=seq(0, 100, by=10)
  log.lkhd=numeric(length(beta.range))
  for (i in 1:length(beta.range))
  log.lkhd[i]=objec.func(beta.range[i])
  plot(beta.range, log.lkhd, type="o", xlab=expression(beta), ylab="log(likelihood)")
  abline(v=beta.true, col=2)
###################################  
  
  
  for (k in 1:length(lambda.range))
  {
    cat(k, "is running", "\n")  
    lambda=lambda.range[k] 
    beta.est=runif(anno.num, -1,1)
    theta.est=optim(beta.est, objec.func, deriv.objec.func, method="BFGS", control=list(fnscale=-1))
    beta.fin.est[k]=theta.est$par
#    theta.est=DEoptim(fn=objec.func.min, lower=rep(-1, anno.num), upper=rep(200, anno.num), control=list(NP=100, itermax=100,trace=FALSE))
#    beta.fin.est[k,]=theta.est$optim$bestmem
    error[k]=sum((beta.fin.est[k]-beta.true)^2)
    
  }
```



### Case 2: Two Annotation features 
$K=2$. Set $\beta_1=0.05, \beta_2=0.7$. Other parameters are the same as in Case 1. 

#### MLE of $\beta$. 

use BFGS method. 

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
