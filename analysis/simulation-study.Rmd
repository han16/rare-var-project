---
title: "Simulation study"
author: "Shengtong Han"
output: html_document
---


<!-- Add your analysis here -->
## Data Generation

Basically, there are two sequential levels when generating the data. First under null hypothesis, generate total number of variants in cases and controls, i.e., AF $q_{ij}^{(0)} \sim Beta(\alpha_0, \beta_0)$, $X_{1ij}+X_{0ij} \sim Pois (N1+N0, q_{ij}^{(0)})$,  filter out variants will null mutations, then re-distribute all mutations in effective variants into cases and controls depending on the risk status via conditional binomial distribution.     

* gene level: $U_i$ denote the risk status of gene $i$, $U_i \sim Ber(1, \delta)$. All genes share the same risk probability $\delta$. 
* variant level:
    +  $U_i=0$, all variants for gene $i$ are non-causal, which is under null hypothesis. Given the generated $X_{1ij}+X_{0ij}$, split into case and controls by conditional binomial distribution, with $p=\frac{N1}{N1+N0}$  
    + $U_i=1$,  gene $i$ is a causal gene, the variant $(i,j)$ whose risk status is denoted by $Z_{ij}$ is generated as $Z_{ij} \sim Ber(1, \pi(\beta))$, $\pi(\beta)$ is a function linking annotations to the probability of being causal. 
          *  $Z_{ij}=0$ (which is under null),  given the generated $X_{1ij}+X_{0ij}$, split into case and controls by conditional binomial distribution, with $p=\frac{N1}{N1+N0}$
          *   $Z_{ij}=1$ (which is under alternative), AF $q_{ij}^{(1)} \sim Beta(\alpha, \beta)$, $\gamma_{ij} \sim Gamma(\bar{\gamma}*\sigma, \sigma)$,  $X_{1ij}+X_{0ij} \sim Pois(N1+N0, q_{ij}^{(1)})$, split into case and control by conditional binomial distribution with $p=\frac{N1*\gamma_{ij}}{N1*\gamma_{ij}+N0}$. 
          
Filtering step: filter variants that have null variant counts in both cases and controls.  

Notations: 

* N1, N0 are sample sizes in cases and controls 
* $q_{ij}$ is allele frequency for variant $(i,j)$
* $\bar{\gamma}$ relative risk 


$\pi(\beta)$ is the link function between annotations and prior probability and the interest is in estimating $\beta$.  

## Disjoint group priors 

### Case 1: All genes are risk genes

Parameter settings: N1=N0=3000, $\alpha=\alpha_0=0.1$, $\beta=2000, \beta_0=1000$. Perform 100 independent runs. 

$\beta_i=0.1$, but the actural $\beta_i$ is varying for each generated data set because of the filtering. 



#### (1) estimate $\beta$, fixed $\delta=1$ 

The plot of $\widehat{\beta}-\beta$ is as below 


![](figure/estimatebetariskgene.png)

Clearly, there is a slight over etimate problem. 


* Without filtering, across 100 replicates, $\widehat{\beta}=0.1406$, SD=0.0058.  


#### (2) estimate both $\beta, \delta$

![](figure/estimatebetadeltariskgene1.png)



![](figure/estimatebetadeltariskgene2.png)


### Comparison with other methods 



```{r, echo=T}
fisher.one=c(0.02, 0.04, 0.1, 0.28, 0.58, 0.97, 1)
tmm.one=c(0.03, 0.19, 0.31, 0.7, 0.94, 1,1)
beta.one=c(0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1)
delta.one=1

fisher.two=c(0.12, 0.32, 0.55, 0.82, 0.89)
tmm.two=c(0.26, 0.65, 0.93, 0.98, 1)
beta.two=c(0.1, 0.2, 0.3, 0.4, 0.5)
delta.two=0.1

par(mfrow=c(1,2))
plot(fisher.one, col=3, ylim=c(0,1), type="o", xaxt='n', xlab=expression(paste(beta)),ylab="Sensitivity", main=bquote(delta == .(delta.one)))
lines(tmm.one, col=2, type="o")
legend(1, 0.8, c("FET", "TMM"), col=c(3,2), lty=c(1,1))
axis.labels <-beta.one
axis(1, at=seq(1,7), labels=axis.labels, cex.axis=0.8)


plot(fisher.two, col=3, ylim=c(0,1), type="o", xaxt='n', xlab=expression(paste(beta)),ylab="Sensitivity", main=bquote(delta == .(delta.two)))
lines(tmm.two, col=2, type="o")
#legend(1, 0.8, c("FET", "TMM"), col=c(1,2), lty=c(1,1))
axis.labels <-beta.two
axis(1, at=seq(1,length(beta.two)), labels=axis.labels, cex.axis=0.8)

```



Parameter settings: $N_1=N_0=3000$, all 100 genes are risk genes and each gene has 100 variants in total in one category. The proportion of risk variants $\beta_i$ is varying at different values. $\alpha_0=0.1, \beta_0=1000$, $\alpha=0.1, \beta=2000$, $\bar{\gamma}=6, \sigma=1$. 


To compare with different methods, significance level of 0.05  is used. At each $\beta$, 100 data sets are generated with the above parameters. For every data set, we essentially tested if $\beta=0$. The sensitivity in the Figure is actually the proportion of data sets with parameters testing not equal to 0, i.e., $\hat{\beta}!=0$ 


```{r, echo=T}
rate.lrt=c(0.36,0.6, 0.79, 0.92, 0.95)
rate.skat=c(0.23, 0.37, 0.51, 0.64, 0.74)
rate.burden=c(0.06, 0.07, 0.18, 0.14, 0.24)
beta=c(0.002,0.004, 0.006, 0.008, 0.01)


plot(rate.burden, col=3, ylim=c(0,1), type="o", xaxt='n', xlab=expression(paste(beta)),ylab="Sensitivity", main="", pch=15)
lines(rate.lrt, col=2, type="o", pch=16)
lines(rate.skat, col=4, type="o", pch=17)
legend(1, 1, c("FET", "LRT", "SKAT"), col=c(3,2, 4), lty=c(1,1, 1), pch=c(15, 16, 17))
axis.labels <-beta
axis(1, at=seq(1,5), labels=axis.labels, cex.axis=0.8)

```

```{r, echo=T}
library(ggplot2)
method=c("FET", "LRT", "SKAT")
beta.comp=rep(beta, 3)
method.comp=rep(method, each=length(beta))
rate.comp=c(rate.burden, rate.lrt, rate.skat)
rate.comb=data.frame(method=method.comp, beta.comp=beta.comp, rate.comp=rate.comp)
#pdf("../../Figure/powercomparison.pdf")
ggplot(rate.comb, aes(x=beta.comp, y=rate.comp, group=method))+
  geom_line(aes(color=method, linetype=method), size=1)+
  geom_point(aes(color=method, shape=method), size=2)+
  xlab(expression(paste(beta)))+ylab("Sensitivity")
#dev.off()
```


### Case 2: Mixture model



## Logistic priors  
### Case 1: One annotation feature
#### Paramster settings 1 : 

* $\delta=1$
* $N1=N0=5000$
* number of genes $I=1000$, number of variants per gene $m=200$ before filtering. 
* $\alpha=\alpha_0=0.1$, $\beta_0=1000, \beta=2000$. 
* $\beta_{true}=0.5$,  $\bar{\gamma}=10$
* 80\% elements of of Ajk are 1

This is the simplest case in that every variant has one annotation feature if any. 


#### likelihood function: 
If the feature has one dimension, the objective likelihood function would be simple to be optimized using common R packages. The log likelihood as a function of $\beta$ (true $\beta=0.5$)is as 



![loglikelihood](figure/loglikelihood.png). 

This plot tells us the likelihood only has one mode in one dimension case. It looks like $\beta$ is overestimated by R package-BFGS. With the fixed randomly generated data, run BFGS 50 times with random initials. The mean of $\hat{\beta}_{MLE}=0.7631$ and SD=2.344796e-05. 


#### Effect of Sparsity of $A_{jk}$ ($k$ is the number of features, here $k=1$) on MLE of $\beta$

Set $\beta_{true}=0.5$, vary the sparsity of $A_{jk}$, i.e., the ratio of 0's in $A_{jk}$. Define sparsity rate as the ratio of 0's in each feature category.  


Sparsity rate | 90% | 80% |70%  |60% |50% |40%  | 30% |20% |10% | 0|
-------------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$ |0.3802 | 0.4499 |0.5845 |0.5420 |0.5829 |0.64444|0.6817|0.7608|0.7941|0.8896|
Table: MLE at different sparsity rate $\beta_{true}=0.5$

When All entries of $A_{jk}$ are zero, $\tau_j=\frac{1}{2}$, regardless of whatever values of $\beta$ and the likelihood function is a constant of $\beta$. Thus $\widehat{\beta}$ can be anywhere.   


Set $\beta_{true}=0.2$.  


Sparsity rate | 95% | 90% |85%  |80% |75% |70%  | 65% |60% |55% | 50%|
-------------|-------|-------|--------|-------|-------|-------|-------|------|------|------|------|
$\hat{\beta}_{MLE}$|0.3075 | 0.2259 |0.3481 |0.3944 |0.3677 |0.4167|0.3956|0.4331|0.4565|0.4091|
Table: MLE at different sparsity rate with  $\beta_{true}=0.2$. 

####Important Note: 
There are two variables affecting estimate of $\beta$, sparsity of $A_{jk}$ and $\beta_{true}$. 
(1) $\beta$ cannot be set too large, say >10 because it controls the probability of a variant of being causal. Consider an extreme case where all variants are truly causal risk variants. The likelihood of the data is close to the product of Bayes factor of every variant, free of $\beta$ value. Any large enough $\beta$ will give the same likelihood. (2) Sparsity of $A_{jk}$ will certainly influences the estimate of $\beta$. The less sparsity, the more information will be used, the more accurate the estimate will be. (3) AF cannot be too large because for common variants with large odds ratio (large $\bar{\gamma}$), Bayes factor will become overflow of being Inf.      

####Parameter setting 2-increasing sample size
* $\delta=1$
* $\beta_{true}=2$, $\bar{\gamma}=10$
* all entries of $A_{jk}$ are 1
* $\alpha=\alpha_0=0.1$, $\beta=1000, \beta_0=2000$



N1=N0|10,000 | 50,000 | 100,000|
--------|-------|--------|-----------------------------------------
 $\widehat{\beta}_{MLE}$ by Optim | 2.7810 (0.0362) | 2.4413 (0.0161)| 2.3420 
$\widehat{\beta}_{MLE}$ by GLM | 2.0016  (0.0126)|
Table: MLE averaging across 10 replicates with different sample size 


To use GLM, $Z_{ij}$ is treated as response (but in practice, $Z_{ij}$ is unknown) and each column of $A_{jk}$ is a predictor, but without intercept.  

Use function GLM to estimate $\beta$. 


### Case 2: Two Annotation features 
$K=2$. Set $\beta_1=0.05, \beta_2=2$. Other parameters are 

* $\delta=1$
* $\bar{\gamma}=10$
* $\alpha=\alpha_0=0.1$, $\beta=1000, \beta_0=2000$

When there are more than 2 groups, $A_{ij}$ must be designed carefully such that all columns (all annotations) have as little overlap as possible. Otherwise, the method have difficulty in distinguishing between them, leading to poor estimate. In simulations below, these two factures have no overlap. 



N1=N0|10,000 | 50,000 | 80,000|
--------|-------|--------|-----------------------------------------
 $\widehat{\beta_1}_{MLE}$ by Optim | 0.3472 (0.0239) | 0.2287 (0.0252)|0.2371 (0.0193)  
 $\widehat{\beta_2}_{MLE}$ by Optim | 2.7807 (0.0919) | 2.4239 (0.0211)| 2.3636 (0.0243)
$\widehat{\beta_1}_{MLE}$ by GLM | 0.0517  (0.0159)| |
$\widehat{\beta_2}_{MLE}$ by GLM | 1.9952  (0.0177)||
Table: MLE averaging across 10 replicates with different sample size 


### Case 3:  Many annotation features


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
